# POC Java Spring AI

## ğŸ¤– POC â€” Spring AI com Ollama (Chat Simples)

### ğŸ“Œ 1ï¸âƒ£ Objetivo

Esta Proof of Concept (POC) demonstra a integraÃ§Ã£o entre:

- Spring AI

- Spring Boot

- Ollama

- Java 21


O objetivo da Fase 1 Ã©:

- Integrar uma LLM local

- Criar um endpoint REST

- Enviar prompts para o modelo

- Retornar resposta via API

### ğŸ—ï¸ 2ï¸âƒ£ Stack TecnolÃ³gica

| Tecnologia   | VersÃ£o          |
|--------------|-----------------|
| Java         | 21 (LTS)        |
| Spring Boot  | 3.3+            |
| Spring AI    | 1.0.0           |
| Maven        | 3.9+            |
| Ollama       | Ãšltima versÃ£o   |
| Modelo       | llama3          |

### ğŸ§  3ï¸âƒ£ Arquitetura Atual (Fase 1)

````
controller
 â””â”€â”€ AiController
````

Fluxo:

````
Client â†’ Controller â†’ ChatClient â†’ Ollama (localhost:11434) â†’ Response
````
### âš™ï¸ 4ï¸âƒ£ ConfiguraÃ§Ã£o

application.yml

````
spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        model: llama3
        options:
          temperature: 0.3
````

### ğŸš€ 5ï¸âƒ£ ExecuÃ§Ã£o

#### 1ï¸âƒ£ Subir Ollama

````
ollama run llama3
````

#### 2ï¸âƒ£ Rodar aplicaÃ§Ã£o

````
mvn spring-boot:run
````

#### 3ï¸âƒ£ Testar endpoint

````
 curl "http://localhost:8080/ai/ask?question=Explique%20o%20que%20%C3%A9%20Spring%20Boot"
````
### ğŸ” 6ï¸âƒ£ Endpoint disponÃ­vel

````
GET /ai/ask?question={pergunta}
````

Exemplo:

````
GET /ai/ask?question=Explique%20Java%20Locks
````

### ğŸ“ˆ 8ï¸ PrÃ³ximas Fases

- Fase 2 â†’ Prompt Template estruturado

- Fase 3 â†’ Memory (conversaÃ§Ã£o contextual)

- Fase 4 â†’ Embeddings

- Fase 5 â†’ RAG (Retrieval Augmented Generation)

- Fase 6 â†’ Tool Calling

- Fase 7 â†’ Streaming

ğŸš€ Fase 2 â€” Prompt Template Estruturado
ğŸ¯ Objetivo
Separar System Prompt e User Prompt

Definir comportamento do modelo

Melhorar qualidade e consistÃªncia das respostas

Introduzir camada de serviÃ§o

### ğŸ—ï¸ Arquitetura Atual (Fase 2)

controller
 â””â”€â”€ AiController

service
 â””â”€â”€ AiService

Fluxo atualizado:

````
Client â†’ Controller â†’ Service â†’ ChatClient â†’ Ollama â†’ Response
````

### ğŸ§  System Prompt Implementado

O modelo agora recebe contexto fixo:

VocÃª Ã© um especialista em Java e Spring.
Responda de forma tÃ©cnica, clara e objetiva.

Isso garante:

- Respostas mais tÃ©cnicas

- PadronizaÃ§Ã£o

- Controle de tom

- Maior previsibilidade


## âš™ï¸ 4ï¸âƒ£ ConfiguraÃ§Ã£o

```
application.yml
spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        model: llama3
        options:
          temperature: 0.3
```

### ğŸš€ 5ï¸âƒ£ ExecuÃ§Ã£o

#### 1ï¸âƒ£ Subir Ollama

```
ollama run llama3
```

#### 2ï¸âƒ£ Rodar aplicaÃ§Ã£o

```
mvn spring-boot:run
```


#### 3ï¸âƒ£ Testar endpoint

````
 curl "http://localhost:8080/ai/ask?question=Explique%20o%20que%20%C3%A9%20Spring%20Boot"
````

### ğŸ” 6ï¸âƒ£ Endpoint disponÃ­vel

```
GET /ai/ask?question={pergunta}
```

Exemplo:

```
GET /ai/ask?question=Explique%20Java%20Locks
```

### ğŸ§ª ObservaÃ§Ãµes TÃ©cnicas

- A primeira chamada pode demorar (cold start do modelo)

- O processamento ocorre 100% local

- A latÃªncia depende de CPU e RAM

- A temperatura controla a criatividade da resposta